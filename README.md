# ğŸ¥ MediTalk for Patient

MediTalk is an AI-powered medical assistant platform designed to assist patients in understanding health documents and interactions. It combines OCR (Optical Character Recognition), text summarization, medical question answering, and speech synthesis into a seamless experience.

---

## âœ¨ Features

- ğŸ“¸ **OCR**: Extract text from medical reports, prescriptions, or documents using Tesseract.
- ğŸ“„ **Summarization**: Get simplified summaries of complex medical text using a transformer-based model (BART).
- â“ **Medical Q&A**: Ask health-related questions and get AI-driven answers based on context.
- ğŸ”¡ **Text-to-Speech (TTS)**: Convert translated or summarized content into speech across multiple languages.
- ğŸŒ **Translation**: Translate input to different languages before speech synthesis.
- ğŸ§  **Language Detection**: Automatically detects the language of input text.

---

## ğŸ“¦ Tech Stack

- **Frontend**: React (TypeScript), Tailwind CSS (or custom CSS), Streamlit (optional prototype UI)
- **Backend**: FastAPI (Python)

### AI/ML:

- `transformers` (HuggingFace) for summarization and QA
- `gTTS` or another TTS engine
- `langdetect` for detecting input language
- `Tesseract OCR` for image-to-text

---

## ğŸ“ Project Structure

```
meditalk/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ ocr.py
â”‚   â”‚   â”œâ”€â”€ tts.py
â”‚   â”‚   â”œâ”€â”€ summarize.py
â”‚   â”‚   â””â”€â”€ qa.py
â”‚   â””â”€â”€ models/ (optional if you're fine-tuning)
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.tsx
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ styles/
â”‚   â”‚   â””â”€â”€ pages/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ package.json
```

---

## âœ¨ Getting Started

### Prerequisites

- Python 3.9+
- Node.js 18+
- Tesseract installed locally (`brew install tesseract` or `sudo apt install tesseract-ocr`)

### Backend Setup

```bash
cd backend
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
pip install -r requirements.txt
python -m uvicorn app.main:app --reload
```

### Frontend Setup

```bash
cd frontend
npm install
npm run dev
```

---

## ğŸ“· OCR Example

Send a base64 image or file to `/ocr/` to extract text.

---

## ğŸ”Š TTS + Translation

POST to `/tts/?target_lang=hi&slow=false` with body:

```json
{
  "text": "Take this medicine twice a day after food."
}
```

Response will include translated text, detected language, and audio URL.

---

## ğŸ” Summarization

POST to `/` (e.g. `/summarize/`) with:

```json
{
  "text": "This is a long medical paragraph that needs summarization."
}
```

---

## ğŸ“– Medical Q&A

POST to `/qa/` with:

```json
{
  "question": "What are the side effects of paracetamol?",
  "context": "Paracetamol may cause liver damage if taken in excess..."
}
```

---

## ğŸ’¡ Use Cases

- Helping patients understand prescriptions
- Voice-assistive accessibility for elderly or visually impaired
- Instant language translation of medical advice
- Offline chatbot mode for remote health support

---

## frontend
https://meditalk-1.onrender.com

## backend can be run over local server as it can't be deployed over free
http://localhost:8000

## ğŸ›¡ï¸ License

MIT License
